# Robust Scraping Orchestrator

A high-performance web scraping solution built with Scrapy and Selenium, containerized with Docker for easy deployment and scaling.

## ğŸš€ Features

- **Web Scraping**: Extracts car listings from AutoRIA with detailed information
- **Headless Chrome**: Uses Selenium with headless Chrome for JavaScript rendering
- **Asynchronous Processing**: Implements a pool of Chrome drivers for concurrent requests
- **PostgreSQL Storage**: Stores scraped data with proper schema and indexing
- **Dockerized**: Easy deployment with Docker and Docker Compose
- **Scheduled Scraping**: Built-in scheduler for periodic data collection
- **Proxy Support**: Configurable proxy settings for reliable scraping
- **User-Agent Rotation**: Prevents blocking with rotating user agents

## ğŸ› ï¸ Prerequisites

- Docker 20.10+
- Docker Compose 2.0+
- Python 3.8+ (for local development)

## ğŸš€ Quick Start

1. **Clone the repository**
   ```bash
   git clone [https://github.com/yourusername/robust-scraping-orchestrator.git](https://github.com/yourusername/robust-scraping-orchestrator.git)
   cd robust-scraping-orchestrator
2. **Set up environment variables**
   ```bash
    cp .env.example .env
    # Edit .env with your configuration
   ```
3. **Build and start the containers**
   ```bash
    docker-compose up --build
   ```
4. **Access the services**
   - Scraper logs: docker logs -f scrapy_selenium
   - Database: postgresql://user:password@localhost:5432/dbname
   - pgAdmin: http://localhost:8080 (if enabled)
   
### ğŸ—ï¸ Project Structure
   

     â”œâ”€â”€ carscraper/               # Scrapy project
     â”‚   â”œâ”€â”€ items.py             # Data models
     â”‚   â”œâ”€â”€ loaders.py           # Data processing
     â”‚   â”œâ”€â”€ middlewares.py       # Custom middlewares
     â”‚   â”œâ”€â”€ pipelines.py         # Data storage
     â”‚   â”œâ”€â”€ settings.py          # Scrapy settings
     â”‚   â””â”€â”€ spiders/             # Spider implementations
     â”‚       â””â”€â”€ carspider.py     # Main spider
     â”œâ”€â”€ docker/                  # Docker configuration
     â”œâ”€â”€ logs/                    # Log files
     â”œâ”€â”€ .env.example             # Example environment variables
     â”œâ”€â”€ docker-compose.yml       # Docker Compose configuration
     â”œâ”€â”€ Dockerfile               # Docker build configuration
     â”œâ”€â”€ requirements.txt         # Python dependencies
     â””â”€â”€ scheduler.py            # Scraping scheduler

### âš™ï¸ Configuration
Configure the application using environment variables in .env:
    
   ```ini
    # Database
    POSTGRES_DB=scrapy
    POSTGRES_USER=user
    POSTGRES_PASSWORD=password
    
    # Scraping
    SCRAPEOPS_API_KEY=your_api_key
    PROXY_URL=http://your-proxy:port
    
    # Scheduler
    SPIDER_TIME=*/30 * * * *  # Run every 30 minutes
    DUMP_TIME=0 0 * * *       # Daily dumps at midnight
   ```
### ğŸ› ï¸ Development
1. **Set up virtual environment**
   ```bash
    python -m venv venv
    source venv/bin/activate  # On Windows: venv\Scripts\activate
    pip install -r requirements.txt
   ```
2. **Run the spider locally**
   ```bash
    scrapy crawl carspider
   ```
3. **Run tests**
   ```bash
    python -m pytest
   ```
### ğŸ“¦ Deployment
   ```bash
   docker-compose --build up -d
   ```
Monitoring
- Logs: docker-compose logs -f
- Database: Use pgAdmin at http://localhost:8080
- Health checks: docker ps to check container status

### ğŸ¤ Contributing
1. Fork the repository
2. Create a feature branch
3. Commit your changes
4. Push to the branch
5. Create a Pull Request

### ğŸ“„ License
This project is licensed under the MIT License - see the LICENSE file for details.
### ğŸ“§ Contact
For support or questions, please contact **Maksym Plakushko** at mplakushko@gmail.com